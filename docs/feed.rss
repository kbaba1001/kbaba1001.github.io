<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" version="2.0">
  <channel>
    <title>kbaba1001 ブログ</title>
    <link>https://www.kbaba1001.com/</link>
    <atom:link href="https://www.kbaba1001.com/feed.rss" rel="self" type="application/rss+xml"/>
    <description>Clojure 好きなプログラマ kbaba1001 のブログ</description>
    <lastBuildDate>Sun, 17 Dec 2023 15:00:00 GMT</lastBuildDate>
    <language>ja_JP</language>
    <generator>Lume v2.0.1</generator>
    <item>
      <title>ローカル LLM を自宅サーバーで動かして Chatbot を作る</title>
      <link>https://www.kbaba1001.com/posts/2024022501_llama-chatbot/</link>
      <guid isPermaLink="false">https://www.kbaba1001.com/posts/2024022501_llama-chatbot/</guid>
      <description/>
      <content:encoded>
        <![CDATA[<p>今までローカル LLM を色々と試してきたわけだが、ついに集大成として Chatbot
        を作るに至った。</p>
        <p><img src="https://www.kbaba1001.com/img/posts/2024022501/poppins1.png" alt="Poppins1"></p>
        <p>結局、結論としては、</p>
        <ul>
        <li>モデル:
        <a href="https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast-instruct">elyza/ELYZA-japanese-Llama-2-13b-fast-instruct</a>
        を使う
        <ul>
        <li>正確には GGUF がほしいので
        <a href="https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf">こちら</a>
        の <code>ELYZA-japanese-Llama-2-13b-fast-instruct-q4_K_S.gguf</code> を使うことにした</li>
        </ul>
        </li>
        <li>サーバー: <a href="https://github.com/abetlen/llama-cpp-python">llama-cpp-python</a> を
        docker で動かす。
        <ul>
        <li><a href="https://github.com/jasonacox/TinyLLM/tree/main/llmserver">TinyLLM</a>
        をめちゃくちゃ参考にした</li>
        <li><a href="https://github.com/vllm-project/vllm">vLLM</a> は設定が悪いのか GPU
        メモリエラーになるのでひとまず諦めた</li>
        </ul>
        </li>
        <li>クライアント: <a href="https://nlux.dev/">nlux</a> というのが React で動くチャットボットの
        UI をすぐに作れるので採用した</li>
        </ul>
        <h2>動かしてみる</h2>
        <p><img src="https://www.kbaba1001.com/img/posts/2024022501/poppins2.png" alt="Poppins2"></p>
        <p>そこそこまともに回答してくれる。</p>
        <h2>ソースコードサンプル</h2>
        <p>そのうち作る。</p>
        ]]>
      </content:encoded>
      <pubDate>Sun, 25 Feb 2024 14:15:00 GMT</pubDate>
    </item>
    <item>
      <title>frp で自宅サーバーを公開する</title>
      <link>https://www.kbaba1001.com/posts/2024022301_frp/</link>
      <guid isPermaLink="false">https://www.kbaba1001.com/posts/2024022301_frp/</guid>
      <description/>
      <content:encoded>
        <![CDATA[<p><a href="https://ngrok.com/">ngrok</a>
        は素晴らしいサービスなのだが本番用の固定ドメインを使おうとするとドメイン毎に毎月
        $15 かかるので何個も作っているとそれなりの出費になるので代替方法として
        <a href="https://github.com/fatedier/frp">frp</a> を使ってみることにした。</p>
        <p>frp は ngrok みたいなトンネリングを自分でやるためのライブラリだ。</p>
        <h2>概要</h2>
        <p><img src="https://www.kbaba1001.com/img/posts/2024022401/frp.png" alt="frp"></p>
        <p>frp にはサーバー用の frps とクライアント用の frpc がある。 frps
        はグローバルIPから安定してアクセスできる場所に置く必要がある。つまり ngrok
        の場合はこの部分をサービスとしてやっているということになる。</p>
        <p>frps のあるサーバーでは 443 ポートなど使いたいポートについては Firewall
        を開けておく必要があるが frpc を置く方のサーバー（つまり自宅サーバーなど）では
        Firewall を設定する必要はなく、 frpc の設定だけやってあれば良い。</p>
        <p>今回やりたいことは２つ</p>
        <ul>
        <li>自宅サーバーにインターネット越しに SSH でログインする</li>
        <li>自宅サーバーからホームページをインターネットに公開する</li>
        </ul>
        <h2>frps の設定</h2>
        <p>以前安かったときに契約した ConoHa のサーバーがあるのでこれを frps
        用に使うことにした。 固定IP が使えて frps
        の実行ファイルが動かせればなんでもいい気がするので、 Cloud Run とか Faas
        とかでもいけるかもしれない。</p>
        <p>frps.toml を次のように設定する。</p>
        <pre><code>bindPort = 7000
        vhostHTTPSPort = 443
        </code></pre>
        <p>次で起動</p>
        <pre><code>./frps -c frps.toml
        </code></pre>
        <p>ポート 7000, 2222, 443 を使うので開けておく。 (Linux の Firewall は最近は ufw
        を使っている。簡単なので便利。)</p>
        <h2>frpc の設定: SSH</h2>
        <p>次に自宅サーバーで次のような設定をしておく。</p>
        <p><code>frpc.toml</code></p>
        <pre><code>serverAddr = &quot;xxx.xxx.xxx.xxx&quot; # ConoHa サーバーのグローバル IP アドレス or 左記を割り当てたドメイン
        serverPort = 7000
        
        [[proxies]]
        name = &quot;ssh&quot;
        type = &quot;tcp&quot;
        localIP = &quot;127.0.0.1&quot;
        localPort = 22
        remotePort = 2222
        </code></pre>
        <p>で、次で起動</p>
        <pre><code>./frpc -c frpc.toml
        </code></pre>
        <p>上記の設定の場合、自宅サーバーでポート 22 で SSH 接続できるようにしておいて、
        frps 経由では 2222 ポートでアクセスできるように設定してある。</p>
        <p>手元のマシンの SSH
        設定を次のようにすればインターネットのどこからでも自宅サーバーにアクセスできる。</p>
        <p><code>~/.ssh/config</code></p>
        <pre><code>host my-server
        HostName xxx.xxx.xxx.xxx
        User alice
        Port 2222
        ForwardAgent yes
        </code></pre>
        <p>User とか IdentityFile の設定とかは適当にやっておく。</p>
        <h2>frpc の設定: https サーバー</h2>
        <p>インターネットにサイトを公開するのであれば HTTPS で公開したい。 frp には
        https2http というプラグインがあってこれを使うと http で動いているローカルの
        アプリケーションサーバーに対して https で公開できるようになる。</p>
        <h3>HTTPS の証明書ファイルの生成</h3>
        <p>今回は HTTPS 化のために <a href="https://letsencrypt.org/">Let's Encrypt</a> を使う。 Let's
        Encrypt は certbot という Linux コマンドで証明書ファイルを作ることができる。</p>
        <pre><code class="language-bash">certbot certonly --manual -d *.kbaba1001.com --preferred-challenges dns --config-dir ./letsencrypt/config --work-dir ./letsencrypt/work --logs-dir ./letsencrypt/logs
        </code></pre>
        <p>こんな感じで設定ファイルを作ることができる。（ディレクトリは予め作っておく）</p>
        <p>ドメインでのTXT設定が求められるのでそれは対応すること。</p>
        <p>上記の場合は <code>./letsencrypt/config/live/kbaba1001.com/cert.pem</code>
        などのような形で証明書ファイルができる。</p>
        <h3>frpc の設定</h3>
        <p>新しい frpc.toml を作って次のように設定する。</p>
        <pre><code>serverAddr = &quot;xxx.xxx.xxx.xxx&quot;
        serverPort = 7000
        vhostHTTPSPort = 443
        
        [[proxies]]
        name = &quot;tunnel_https2http&quot;
        type = &quot;https&quot;
        customDomains = [&quot;tunnel.kbaba1001.com&quot;]
        
        [proxies.plugin]
        type = &quot;https2http&quot;
        localAddr = &quot;127.0.0.1:8000&quot;
        crtPath = &quot;./letsencrypt/config/live/kbaba1001.com/cert.pem&quot;
        keyPath = &quot;./letsencrypt/config/live/kbaba1001.com/privkey.pem&quot;
        hostHeaderRewrite = &quot;127.0.0.1&quot;
        requestHeaders.set.x-from-where = &quot;frp&quot;
        </code></pre>
        <p><code>frpc -c frpc.toml</code> で起動しておいて、ブラウザから
        <code>https://tunnel.kbaba1001.com</code> にアクセスすれば 通信が Proxy
        されて自宅サーバーの localhost:8000 にリクエストが飛ぶようになる。</p>
        <h2>まとめ</h2>
        <p>frps
        を動かすサーバーが必要になるものの、お金をあまりかけずに自宅サーバーを公開するには良い方法だと思う。</p>
        ]]>
      </content:encoded>
      <pubDate>Fri, 23 Feb 2024 23:30:00 GMT</pubDate>
    </item>
    <item>
      <title>最近買った本</title>
      <link>https://www.kbaba1001.com/posts/2024022301_books/</link>
      <guid isPermaLink="false">https://www.kbaba1001.com/posts/2024022301_books/</guid>
      <description/>
      <content:encoded>
        <![CDATA[<h2>入門コンピュータ科学</h2>
        <p><img src="https://www.kbaba1001.com/img/posts/2024022301/computer-book.jpg" alt=""></p>
        <p>妻がコンピュータの歴史や仕組みが知りたいというので買ってみた。
        パラパラめくってみた感じ、大学の情報工学科で習うようなことが
        一通り書いてある感じだった。</p>
        <p>基本情報処理技術者試験とか好きな人には良さそう。</p>
        <h2>オイラーの贈物、数学ガール</h2>
        <p><img src="https://www.kbaba1001.com/img/posts/2024022301/ei-pi-book.jpg" alt=""></p>
        <p>オイラーの公式について復習したくて買ってみた。 複素平面っていいよね。
        想像上の数字に過ぎない複素数が現実世界に影響を与えているのが良い。</p>
        ]]>
      </content:encoded>
      <pubDate>Fri, 23 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>掲示板を作り、既存のコメントシステムの廃止しました</title>
      <link>https://www.kbaba1001.com/posts/2024022103_bbs/</link>
      <guid isPermaLink="false">https://www.kbaba1001.com/posts/2024022103_bbs/</guid>
      <description/>
      <content:encoded>
        <![CDATA[<p>前からリクエストが多かった<a href="https://github.com/kbaba1001/kbaba1001.github.io/discussions">掲示板機能</a>を作りました。</p>
        <p>リンク先を見れば分かる通り、 GitHub Discussions です。
        そういえばこんな機能できてたなぁと思ったので使えるようにしました。</p>
        <p>前々からマシュマロや google form でのコメントがいまいちだなぁと思っていたので、
        GitHub Discussions に気がついたのは良い考えだなと我ながら思います。
        なにせお金がかかりませんし、システムのメンテナンスの必要もありません。
        そもそもこのブログ自体も GitHub Pages で動いてますので相性抜群です。
        情報の集約にもなります。</p>
        <p>というわけで今までのマシュマロおよび google form でのコメント機能は廃止します。
        今後は上記の掲示板にメッセージを頂けたら幸いです。</p>
        <p>GitHub アカウントがない方はどうするか？ 作ってください。
        アカウントがなくても閲覧のみならできます。</p>
        ]]>
      </content:encoded>
      <pubDate>Wed, 21 Feb 2024 16:15:00 GMT</pubDate>
    </item>
    <item>
      <title>github template repository を作りたい</title>
      <link>https://www.kbaba1001.com/posts/2024022102_github-template-repository/</link>
      <guid isPermaLink="false">https://www.kbaba1001.com/posts/2024022102_github-template-repository/</guid>
      <description/>
      <content:encoded>
        <![CDATA[<p>今更ながら
        <a href="https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-template-repository">github template repository</a>
        なるものを知った。
        リポジトリを作るときにテンプレートを選ぶと初期セットアップできるようだ。</p>
        <p>便利じゃん。。。</p>
        <p>今まで <a href="https://github.com/neumann-tokyo">ノイマントーキョーのリポジトリ</a> に
        <code>*-init</code> の名前で 初期セットアップリポジトリを作ってきていたのでこれらを
        template repository 化しようと思う。 正直、あんま使ってないのもあるけど。</p>
        <p>npm
        コマンドとかでテンプレート化しようと思っていたけど、ちょっと面倒くさいしなぁと思っていたので
        github template repository は良さそう。 npm パッケージとして公開しなくていいし、
        Clojure などの言語でも使えるし。 というか Clojure こそテンプレート必要では...。</p>
        <h2>共通化しておきたいもの</h2>
        <p>Webシステムを使うときに次のものはだいたい必要になるのでひと通り揃えておきたい。</p>
        <ul>
        <li>[サーバー側] データベースへのアクセスと SQL Builder 的なライブラリ</li>
        <li>[サーバー側] REST API (または RPC) 構築用のルーティングライブラリ</li>
        <li>[サーバー側] <a href="https://github.com/golang-migrate/migrate">migrate</a>
        によるデータベースのマイグレーション</li>
        <li>[サーバー側] データベースの seeds データの読み込みの仕組み</li>
        <li>[フロント側] React, jotai,
        <a href="https://github.com/jotaijs/jotai-tanstack-query">jotai-tanstack-query</a>,
        ルーティング (<a href="https://github.com/molefrog/wouter">Wouter</a> か
        <a href="https://reactrouter.com/en/main">ReactRouter</a> かなぁ), Chakra UI</li>
        <li>[フロント側] REST Client (または RPC Client) のセットアップ</li>
        <li>formatter, linter, および git pre-commit で左記を自動実行するための仕組み
        (<a href="https://github.com/evilmartians/lefthook">lefthook</a> みたいなやつ)
        <ul>
        <li>最近だと <a href="https://biomejs.dev/">Biome</a> が気に入っているのだが Deno
        プロジェクトだったら標準の deno fmt とかでも良さそうなので悩みどころ</li>
        </ul>
        </li>
        <li><a href="https://code.visualstudio.com/docs/devcontainers/containers">devcontainer</a>
        による実行環境</li>
        <li>docker-compose によるデータベースなどのセットアップ</li>
        <li>テスト関係</li>
        <li>vite などのビルド系ツールのセットアップ</li>
        <li>基本的な機能
        <ul>
        <li>JWT を用いた email/password による認証機能</li>
        <li>認可機能</li>
        <li>上記の管理機能</li>
        </ul>
        </li>
        </ul>
        ]]>
      </content:encoded>
      <pubDate>Wed, 21 Feb 2024 04:32:00 GMT</pubDate>
    </item>
    <item>
      <title>Windowsの「ペイント」の進歩がすごい</title>
      <link>https://www.kbaba1001.com/posts/2024022101_paint/</link>
      <guid isPermaLink="false">https://www.kbaba1001.com/posts/2024022101_paint/</guid>
      <description/>
      <content:encoded>
        <![CDATA[<p>久々にペイントを起動したらめちゃくちゃ機能が増えていて驚いた。</p>
        <p><img src="https://www.kbaba1001.com/img/posts/2024022101/paint01.png" alt="ペイント"></p>
        <p>なんかレイヤーとか使えるし。
        図形描画機能もイラストレーターみたいに後からストロークや色などを変えることができる。
        ベクターデータになっているということだろうか？</p>
        <h3>背景を自動削除できる</h3>
        <p>驚いたのがこの機能。</p>
        <p><img src="https://www.kbaba1001.com/img/posts/2024022101/paint02.png" alt="ペイント"></p>
        <p>こんな感じの写真があったときに、</p>
        <p><img src="https://www.kbaba1001.com/img/posts/2024022101/paint03.png" alt="ペイント"></p>
        <p>左上の赤丸のボタンを押すとワンクリックで背景を消してくれる。</p>
        <p>すごい。 かなりきれいに消えてるから活用方法ありそう。</p>
        <h3>AI 生成</h3>
        <p><a href="https://www.lifehacker.jp/article/2402-how-to-create-ai-art-in-microsoft-paint-with-cocreator/">Windowsのペイント、ついに画像生成AIを搭載。その名も「Cocreator（コクリエーター）」</a>
        によると画像生成機能がつくらしい。</p>
        <p>すごい。</p>
        <p>ある程度無料で使えるみたいだし、めちゃくちゃいいじゃん。</p>
        ]]>
      </content:encoded>
      <pubDate>Wed, 21 Feb 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>実写版映画『ゴールデンカムイ』を見てきた</title>
      <link>https://www.kbaba1001.com/posts/2024021401_golden-kamuy/</link>
      <guid isPermaLink="false">https://www.kbaba1001.com/posts/2024021401_golden-kamuy/</guid>
      <description/>
      <content:encoded>
        <![CDATA[<iframe width="560" height="315" src="https://www.youtube.com/embed/2loIAVv7GYQ?si=wWUiwoibaFyC_hh-" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        <p>実写版映画『<a href="https://kamuy-movie.com/">ゴールデンカムイ</a>』を見てきた。</p>
        <div class="post-learge-font">
        ほんま、ありがとう！ 最高でした
        </div>
        <p>原作の良いシーンたくさん入ってたし、全体的にクオリティ高いし。
        メインキャラはもちろんのこと脇役もチョイ役も原作っぽさがあってとにかく最高。</p>
        <p>今まで見た漫画原作の実写化の中で一番良かったかもしれない。</p>
        <p>もちろん、熊も出てくる。</p>
        ]]>
      </content:encoded>
      <pubDate>Wed, 14 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>日本語ローカル LLM 「ELYZA」と vLLM を試す</title>
      <link>https://www.kbaba1001.com/posts/2024021301_japanese-local-llm2/</link>
      <guid isPermaLink="false">https://www.kbaba1001.com/posts/2024021301_japanese-local-llm2/</guid>
      <description/>
      <content:encoded>
        <![CDATA[<p>前回の
        <a href="https://www.kbaba1001.com/posts/2024020902_japanese-local-llm/">【悲報】日本語ローカル LLM がアホすぎる件</a>
        から次のことを試した。</p>
        <ul>
        <li>一番頭のいいモデルである
        <a href="https://huggingface.co/tokyotech-llm/Swallow-70b-hf">Swallow-70b-hf</a> を試す</li>
        <li><a href="https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast-instruct">elyza/ELYZA-japanese-Llama-2-13b-fast-instruct</a>
        を試す</li>
        <li><a href="https://github.com/vllm-project/vllm">vLLM</a> を試す</li>
        </ul>
        <h2>Swallow-70b-hf</h2>
        <p>自分の GPU (GeForce GTX 1080 Ti)
        では重すぎて１時間待っても3文字くらいしか応答がなかった。
        ちょっと実用性がないので採用を諦めた。</p>
        <h2>ELYZA-japanese-Llama-2-13b-fast-instruct</h2>
        <p>Swallow は前回かなり残念な感じだったので別のモデルを試すことにした。 そこで
        ELYZA 社の上記のモデル。</p>
        <p><img src="https://www.kbaba1001.com/img/posts/2024021301/elyza.png" alt="ELYZA"></p>
        <div class="post-learge-font">
        まともな回答してる！
        <p>これだよ、これ！！</p>
        </div>
        <p>前回の Swallow は何だったのかと思うほどきちんとした回答が返ってきた。
        応答速度も数分かかるものの許容範囲レベル。 ひとまずモデルとしては
        ELYZA-japanese-Llama-2-13b-fast-instruct で決定することにした。</p>
        <h2>vLLM による高速化</h2>
        <p>vLLM はローカル LLM を高速化できるライブラリ。</p>
        <p><a href="https://docs.vllm.ai/en/latest/serving/deploying_with_docker.html">vllm/vllm-openai</a>
        という Docker イメージがあるので試しに使ってみたが、 GPU
        メモリーリークを起こして動かなかった。 自分の GPU
        が貧弱なためか、設定を変える必要があるのかいまいちよくわからない。</p>
        <p>ノート PC の GPU でもメモリーリークしたので多分設定を変える必要がある。</p>
        <p>引き続き試すとして、一旦は llama-cpp-python で動かすことにする。</p>
        ]]>
      </content:encoded>
      <pubDate>Tue, 13 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Google アカウントからもメッセージを送れるようになりました</title>
      <link>https://www.kbaba1001.com/posts/2024020901_add-google-form/</link>
      <guid isPermaLink="false">https://www.kbaba1001.com/posts/2024020901_add-google-form/</guid>
      <description/>
      <content:encoded>
        <![CDATA[<p>試験的に Google Form でのメッセージも受け付けるようにしました。</p>
        <p>↓の「Google アカウントからメッセージを送る」からどうぞ。</p>
        <p>よろしくお願いします。</p>
        ]]>
      </content:encoded>
      <pubDate>Fri, 09 Feb 2024 00:00:00 GMT</pubDate>
    </item>
    <item>
      <title>【悲報】日本語ローカル LLM がアホすぎる件</title>
      <link>https://www.kbaba1001.com/posts/2024020902_japanese-local-llm/</link>
      <guid isPermaLink="false">https://www.kbaba1001.com/posts/2024020902_japanese-local-llm/</guid>
      <description/>
      <content:encoded>
        <![CDATA[<p>ChatGPT を活用した Chatbot 制作の話はよく聞くのだが、それならローカル LLM
        でも作れるんじゃないかと思ってやってみた話。</p>
        <h2>ローカル LLM の世界</h2>
        <p>ローカルで動作する LLM は色々あるっぽいけど今回は Meta 社が作っている
        <a href="https://llama.meta.com/">Llama 2</a> 系で実験することにした。</p>
        <p>Llama 2 は gpt-3.5 レベルの性能があるらしい。</p>
        <p>しかし、日本語対応してないので東工大が作っている
        <a href="https://huggingface.co/tokyotech-llm">Swallow</a> を使うことにした。
        詳しいドキュメント:
        <a href="https://zenn.dev/tokyotech_lm/articles/d6cb3a8fdfc907">Zenn - Swallow: LLaMA-2 日本語継続事前学習モデル</a></p>
        <h2>ローカル LLM の実行</h2>
        <p><a href="https://huggingface.co/tokyotech-llm/Swallow-7b-hf">Swallow の README</a>
        にあるように Python でコードを書いてローカル LLM を動かしても良いのだが、
        少々煩雑だと思う。</p>
        <p>できれば Web API の形で呼び出したい。調べたらちょうどよいのがあった。</p>
        <ul>
        <li><a href="https://github.com/jasonacox/TinyLLM">TinyLLM</a></li>
        </ul>
        <p>これは Llama 2 系のローカル LLM を Chatbot として動くように少々手を加えたもの。</p>
        <p>実質内部で使っているのは</p>
        <ul>
        <li><a href="https://llama-cpp-python.readthedocs.io/en/latest/">llama-cpp-python</a></li>
        <li><a href="https://docs.vllm.ai/en/latest/">vLLM</a></li>
        </ul>
        <p>なので、詳しくなってきたらこの辺を直接使うつもり。 似たようなプロジェクトで
        <a href="https://ollama.ai/">Ollama</a> というのもある。</p>
        <h2>TinyLLM (llama-cpp-python) を動かすための準備</h2>
        <p>TinyLLM (というか llama-cpp-python ) でモデルを使うために Swallow
        のリポジトリから gguf ファイルを作る必要がある。</p>
        <p>変換器を作ってくれている人がいたのでこれを使うことにした。</p>
        <ul>
        <li><a href="https://github.com/3eeps/cherry-py">cherry-py</a> の convert_hf_to_gguf.py</li>
        </ul>
        <pre><code class="language-bash">pip install gguf numpy torch sentencepiece
        python convert_hf_to_gguf.py ../Swallow-7b-hf/ 0
        </code></pre>
        <p>みたいな感じで動かすことができる。</p>
        <h2>TinyLLM (llama-cpp-python) を動かす</h2>
        <p>llama-cpp-python を動かす用の docker-compose.yml
        を書いたりしてよしなに動かした。</p>
        <p>TinyLLM には <a href="https://github.com/jasonacox/TinyLLM/blob/main/chat.py">chat.py</a>
        という動作確認用のファイルが用意されているのでこれを日本語にして使うことにした。</p>
        <p>で、実行結果がこちら：</p>
        <p><img src="https://www.kbaba1001.com/img/posts/2024020902/swallow-llm.png" alt="swallow-llm.png"></p>
        <div class="post-learge-font">
        Σ(･ω･ﾉ)ﾉ ！？
        </div>
        <p>いや、2 ちゃんねるじゃねぇか。。。</p>
        <p>事前にベースのプロンプトとして次の文字を入れてある。</p>
        <pre><code>&quot;あなたの名前はポピンズです。あなたは非常に知的なアシスタントです。
        回答は簡潔かつ正確に答えてください。現在時刻は2024年02月09日です。&quot;
        </code></pre>
        <p>その上で、 <code>あなたの名前はなんですか？</code>
        と聞いた際の回答が上記なのでひどすぎる。。。 会話になってないし。</p>
        <p>今回使用している
        <a href="https://huggingface.co/tokyotech-llm/Swallow-7b-hf">Swallow-7b-hf</a>
        は一番頭が悪いモデルではあるものの、この結果はちょっと残念すぎる。。。
        もう少しまともに動いてほしかった。</p>
        <p>というか思いっきり 2 ちゃんねるの ID
        とか投稿日とか見えてるけどいいのかこれは。。。</p>
        <p>色々と思うことがあるけど残念な気持ちだ。</p>
        <h2>次はどうするか</h2>
        <ul>
        <li>一番頭のいいモデルである
        <a href="https://huggingface.co/tokyotech-llm/Swallow-70b-hf">Swallow-70b-hf</a>
        を試す予定</li>
        <li>動作が早くなるらしいので vLLM も試したい</li>
        <li>Python に詳しくないので JavaScript から実行するのも試したい</li>
        </ul>
        <p>続き:
        <a href="https://www.kbaba1001.com/posts/2024021301_japanese-local-llm2/">日本語ローカル LLM 「ELYZA」と vLLM を試す</a></p>
        ]]>
      </content:encoded>
      <pubDate>Fri, 09 Feb 2024 00:00:00 GMT</pubDate>
    </item>
  </channel>
</rss>