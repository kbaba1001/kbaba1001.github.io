<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" version="2.0">
  <channel>
    <title>ハッカーと漫画家</title>
    <link>https://www.kbaba1001.com/</link>
    <atom:link href="https://www.kbaba1001.com/feed.rss" rel="self" type="application/rss+xml"/>
    <description>Clojure 好きなプログラマ kbaba1001 のブログ</description>
    <lastBuildDate>Sun, 17 Dec 2023 15:00:00 GMT</lastBuildDate>
    <language>ja_JP</language>
    <generator>Lume v2.0.1</generator>
    <item>
      <title>技術書典18にサークル「さ19：ノイマンパブリッシング」として参加します</title>
      <link>https://www.kbaba1001.com/posts/202503301121_techbookfest18/</link>
      <guid isPermaLink="false">https://www.kbaba1001.com/posts/202503301121_techbookfest18/</guid>
      <content:encoded>
        <![CDATA[<p><a href="https://techbookfest.org/event/tbf18">技術書典18</a> にサークル名「ノイマンパブリッシング」として参加します。場所は「さ１９」です。</p>
        <p><img src="https://www.kbaba1001.com/img/posts/202503301121/tech18-place.jpg" alt="place"></p>
        <h2>頒布予定の本</h2>
        <ul>
        <li>誰も教えてくれなかったビデオ会議システムの作り方（新刊）
        <ul>
        <li>TypeScript でオンラインミーティングツールを作ります</li>
        </ul>
        </li>
        <li>自宅サーバー関係のエッセイ漫画（Assam著）（新刊）</li>
        <li>自宅サーバーでグループウェアを動かす本（既刊）</li>
        </ul>
        <p>新刊に関しては絶賛執筆中です。
        というよりビデオ会議システムを作っていることろです。
        イベントは６月１日なので４月中旬にはシステムを完成させて、５月中旬までに執筆を終えたいです。
        かなりのボリュームの本になりそうなので果たして間に合うのか。。。</p>
        ]]>
      </content:encoded>
      <pubDate>Sun, 30 Mar 2025 11:21:49 GMT</pubDate>
    </item>
    <item>
      <title>このブログをリニューアルしたい</title>
      <link>https://www.kbaba1001.com/posts/202503291209_idea-of-new-blog/</link>
      <guid isPermaLink="false">https://www.kbaba1001.com/posts/202503291209_idea-of-new-blog/</guid>
      <content:encoded>
        <![CDATA[<p>表題の通りだが、ブログのリニューアルを考えている。</p>
        <p>現状、 <a href="https://lume.land/">Lume</a> で静的なHTMLを生成して Github Pages で公開しているのだが、
        流石にファイルが増えてきたし読者からの反応を得るための機能をなにか入れたくなってきた。</p>
        <p>リニューアル後の想定:</p>
        <ul>
        <li>TwitterみたいなMicropost機能を入れたい</li>
        <li>読者からのリアクション（絵文字とかスタンプとか）を得られるようにしたい</li>
        <li>読者からのコメント機能（一旦設置して様子見。治安悪かったら削除）</li>
        <li>管理画面</li>
        </ul>
        <p>みたいなことがしたいので、 Headless CMS の利用を検討している。</p>
        <p>Headless CMS は管理画面とAPIが用意されていて、フロントは自分でReactとかで作れるというもの。</p>
        <ul>
        <li>無料で使える</li>
        <li>セルフホストできる</li>
        <li>できればNodeなどの自分が知っている言語製</li>
        </ul>
        <p>という条件で考えた結果、<a href="https://strapi.io/">Strapi</a> を使ってみようと思っている。</p>
        <p>今回はちゃんとデータ移行したいけどどうやろうねぇ。
        Markdownと画像ファイルのパスだけDBに入れるような仕組みが作れるといいんだけど。</p>
        ]]>
      </content:encoded>
      <pubDate>Sat, 29 Mar 2025 12:09:14 GMT</pubDate>
    </item>
    <item>
      <title>君は4つのMacbookAirを手に入れた</title>
      <link>https://www.kbaba1001.com/posts/202503191106_4-macbook-air/</link>
      <guid isPermaLink="false">https://www.kbaba1001.com/posts/202503191106_4-macbook-air/</guid>
      <content:encoded>
        <![CDATA[<h2>ジャンクマシン</h2>
        <p>ジャンク品の Macbook Air を知人とかメルカリとかから１台１万円くらいで譲り受けた。
        なんと４台もある。</p>
        <p>実は古い Macbook は非公式の互換バッテリーが Amazon で 5,000 ～ 7,000円くらいで売っているので
        自力で交換することにした。</p>
        <p>ジャンク品だと電源ケーブルもなかったり、あっても使い古されたりしているので
        Amazonで互換品を買うことにした。</p>
        <p>そうすると、予算感としては１台あたり、</p>
        <ul>
        <li>本体: 10,000円</li>
        <li>バッテリー: 6,000円</li>
        <li>電源: 4,000円</li>
        </ul>
        <p>で、約2万円。</p>
        <p>これでそこそこの性能のマシンが手に入るなら安いか。</p>
        <h2>どう使うか</h2>
        <p>Macbook Air には Linux を入れてサーバー or ちょっとした持ち運びマシンにする予定。</p>
        <p>今使っているノートパソコンはゲーミングPCなので、重量が重いし電池のもちが悪い…。
        悪いマシンではないのだが持ち歩くには辛いのだった。</p>
        <p>そこで MacbookAir を Linux 化して SSH やリモートデスクトップでアクセスできるようにしておけば便利かなと思った。</p>
        <p>あるいは Kubernetes クラスタを作ってもいい。</p>
        <h2>Gentoo Linux を入れる</h2>
        <p>妻がOSを学びたいというので、それなら <a href="https://www.gentoo.org/">Gentoo Linux</a> から始めようという話になった。</p>
        <p>自分も Gentoo Linux を使うのは久々なので一緒にやってみる予定。</p>
        ]]>
      </content:encoded>
      <pubDate>Wed, 19 Mar 2025 11:06:39 GMT</pubDate>
    </item>
    <item>
      <title>水彩画修行</title>
      <link>https://www.kbaba1001.com/posts/202502281007_water-color-drawing/</link>
      <guid isPermaLink="false">https://www.kbaba1001.com/posts/202502281007_water-color-drawing/</guid>
      <content:encoded>
        <![CDATA[<p>ほぼ毎日のように水彩画を描いている。</p>
        <h3>夜景</h3>
        <p><img src="https://www.kbaba1001.com/img/posts/202502281007/image_1.jpg" alt="image_1.jpg"></p>
        <p>ぱくたその写真を元に書いた絵。夜の景色のキラキラした感じを描きたかった。</p>
        <h3>日本庭園</h3>
        <p><img src="https://www.kbaba1001.com/img/posts/202502281007/image_2.jpg" alt="image_2.jpg"></p>
        <p>らくがき。草の描き方に課題を感じ始めた。</p>
        <h3>温泉と美女</h3>
        <p><img src="https://www.kbaba1001.com/img/posts/202502281007/image_3.jpg" alt="image_3.jpg"></p>
        <p>これもぱくたその写真から。</p>
        <p>松より手前の水面を詳細に描くべきだったとか色々反省があるけど、
        人物はきれいに書けて満足している。</p>
        <h3>森の少女</h3>
        <p><img src="https://www.kbaba1001.com/img/posts/202502281007/image_4.jpg" alt="image_4.jpg"></p>
        <p>「背景を雑に、前景を丁寧に」がテーマになってきているが、
        この絵を書いたとき胃腸炎でとても体調が悪くて細かいところをうまくかけていない。</p>
        <p>しかしなんとなく成功に向かっている感じはある。</p>
        <h3>りんごとバナナ</h3>
        <p><img src="https://www.kbaba1001.com/img/posts/202502281007/image_5.jpg" alt="image_5.jpg"></p>
        <p>カラーデッサン。</p>
        <h3>似顔絵</h3>
        <p><img src="https://www.kbaba1001.com/img/posts/202502281007/image_6.jpg" alt="image_6.jpg">
        <img src="https://www.kbaba1001.com/img/posts/202502281007/image_7.jpg" alt="image_7.jpg"></p>
        <p>誰の絵っていうのはあえて書かないけど、有名人の似顔絵。
        もちろん非公式のファンアート。</p>
        <p>だいぶ水彩で色を置く感覚に慣れてきたところ。</p>
        <h3>オオハシ</h3>
        <p><img src="https://www.kbaba1001.com/img/posts/202502281007/image_8.jpg" alt="image_8.jpg"></p>
        <p>南国をイメージして描いたオオハシ。
        鳥は丁寧に、背景の木々はやや雑に描くというのがうまくできた気がする。</p>
        <p>こういうタッチや明暗の強弱をつけるのをもっとやりたい。</p>
        <h3>ギターを引く女性</h3>
        <p><img src="https://www.kbaba1001.com/img/posts/202502281007/image_9.jpg" alt="image_9.jpg"></p>
        <p>またしてもぱくたそベースの絵。
        ギター＋女性＋イラストというとどうしても僕の中では江口寿史さんが思い浮かぶので、
        線画をしっかりとペンで書いてからややフラットな感じで着色した。
        ギターとかバイクとかはフリーハンドでもそれなりに描けるようになってきた。</p>
        <h2>動画活動</h2>
        <ul>
        <li><a href="https://youtube.com/shorts/zdIm2FfrOMM?feature=share">ショート動画１</a></li>
        <li><a href="https://youtube.com/shorts/RS-PluvWlJ0?feature=share">ショート動画２</a></li>
        </ul>
        <p>X や Instagram を再開する気にもなれないので YouTube でお絵かき動画を配信することにした。
        まずはショートで知名度を稼ぎたい。</p>
        <h2>紙について</h2>
        <p>F6のコットマンを使っていたが、どうもコットマンの若干水をハネる感じが好きになれず、
        ホワイトワトソンに切り替えた。
        大きさも F6 から F4 にしてやや小さくした。
        「絵がうまくなりたかったら大きい紙に描くように」と昔デッサン教室で言われたことが呪いになっていて、
        あまり小さい紙に描くのが好きではないのだが、F4は結構気楽に書けて良い。
        もともとはカバンに入れて持ち運ぶように買ったのだが、家でも結構使っている。</p>
        <p>しかしやはりある程度ちゃんと描こうとしたらF6くらいの大きさは最低限欲しい感じがする。
        ましてや飾る前提の絵ならもっと大きいほうが…。
        となるとなかなか机の大きさとか保管場所とかに困るのであった。</p>
        ]]>
      </content:encoded>
      <pubDate>Fri, 28 Feb 2025 10:07:42 GMT</pubDate>
    </item>
    <item>
      <title>透明水彩の練習</title>
      <link>https://www.kbaba1001.com/posts/202501300821_practice-of-water-color-painting/</link>
      <guid isPermaLink="false">https://www.kbaba1001.com/posts/202501300821_practice-of-water-color-painting/</guid>
      <content:encoded>
        <![CDATA[<h2>透明水彩によるカラーデッサン</h2>
        <p><img src="https://www.kbaba1001.com/img/posts/202501300821/color1.jpg" alt="color1"></p>
        <p><img src="https://www.kbaba1001.com/img/posts/202501300821/color2.jpg" alt="color2"></p>
        <p>こんな感じで透明水彩を練習中。
        濃い色を出すのが思ったより難しいが、慣れてきた。
        ただ、自分の性格上パキッとした絵を描きたいので、
        不透明水彩のほうが向いている気がする。</p>
        <p>水彩紙も手持ちのものがすべて細目だったので、中目くらいのやつを買っておきたい。</p>
        <h2>モノクロデッサン</h2>
        <p><img src="https://www.kbaba1001.com/img/posts/202501300821/monochrome.jpg" alt="monochrome"></p>
        <p>鉛筆デッサンも相変わらずやっている。</p>
        ]]>
      </content:encoded>
      <pubDate>Thu, 30 Jan 2025 08:21:32 GMT</pubDate>
    </item>
    <item>
      <title>日常にあるものでデッサンをする</title>
      <link>https://www.kbaba1001.com/posts/202501200854_sketching-commodities/</link>
      <guid isPermaLink="false">https://www.kbaba1001.com/posts/202501200854_sketching-commodities/</guid>
      <content:encoded>
        <![CDATA[<h2>日用品のデッサン</h2>
        <p><img src="https://www.kbaba1001.com/img/posts/202501200854/sketching.jpg" alt="日常スケッチ"></p>
        <p>デッサンの入門書を見ると石膏の三角錐などがサンプルに出てくることもあるが、
        そんなの家にないので日常的に目にするものをデッサンするのが
        いいと思っている。</p>
        <p>例えば、カフェの机の上。</p>
        <p>マグカップ、透明なコップ、おしぼり。この３つでも質感の違いを意識して
        デッサンの練習ができる。</p>
        <p>ちょっとしたポットや電球などの工業製品も練習に向いているし、
        空き缶やペットボトルを描いてみるだけでもいい。</p>
        <p>今年も絵をたくさん描きたいので、こういうウォーミングアップを大切にしたいと思う。</p>
        <h2>ヌードデッサン会に行ってきた</h2>
        <p><img src="https://www.kbaba1001.com/img/posts/202501200854/pose01.jpg" alt="ポーズ1">
        <img src="https://www.kbaba1001.com/img/posts/202501200854/pose02.jpg" alt="ポーズ2"></p>
        <p>町中のヌードデッサン会が単発参加できたので行ってきた。
        こういうイベントが多いのは都会という感じがする。正直助かる。</p>
        <p>今まで人物はだいぶ描いてきたが、やはり短い時間で描くのはやはり難しい。</p>
        <p>最近の考えとしては、その人物の雰囲気やポーズをどう捉えるかが案外重要だなと思っている。
        例えば１枚目の寝ているポーズは、背中から足にかけてのゆったりとした曲線美を意識していて、
        最初にそれを下書きしたあと人物として描くようにした。
        ある程度人体のパーツの大きさや描き方などを覚えたら、
        このような概念を意識してみると描きやすくなる気がする。</p>
        <p>参加したデッサン会は意外と初心者が多くてベテランは少なかった。
        自分もすっかりベテラン側かなと思うが、どちらかと言うと自分が一番下手なくらいの回に参加するほうが学びがあって面白い。
        こういう町中のデッサン会に以前参加したのはもう１０年以上前なので、
        その頃は周りの画力に驚いたものだがやはり自分の画力も上がっているし、
        当時ほどの驚きというか新鮮さはなかった。</p>
        <p>なんかもう、そもそも持ってきている道具を見るだけでもある程度その人の画力がわかるようになってしまった。
        昔わからなかったことが色々とわかるようになっているのを認識した。</p>
        ]]>
      </content:encoded>
      <pubDate>Mon, 20 Jan 2025 08:54:40 GMT</pubDate>
    </item>
    <item>
      <title>2025年 新年</title>
      <link>https://www.kbaba1001.com/posts/202501071500_new-year-2025/</link>
      <guid isPermaLink="false">https://www.kbaba1001.com/posts/202501071500_new-year-2025/</guid>
      <content:encoded>
        <![CDATA[<p>新年あけましておめでとうございます。
        今年も当ブログをよろしくお願い致します。</p>
        <h3>引っ越し</h3>
        <p>私事ですが、2024年末に引っ越しをしました。
        静岡から東京に戻った形となります。
        コロナも落ち着いてオフラインイベントも増えてきたことで
        何かと東京の用事も増えてきましたし、
        一方静岡の家では路線バスが大幅に減って生活の不便が増えてきており、
        夫婦で話し合った結果、東京に引っ越そうということになりました。</p>
        <h3>個人開発</h3>
        <p>去年から作っている個人開発システムがいくつかあり、
        引っ越しなどでバタバタしていた関係でどれも中途半端な状態となっているので、
        折を見て再開したいなという気持ちです。</p>
        <p>具体的には次は終わらせたいところ</p>
        <ul>
        <li>ローカル LLM と音声通話するシステムの仕上げ</li>
        <li>FAMと呼んでいるシステムの継続開発 or 作り直し</li>
        </ul>
        <p>一方で <a href="https://zenn.dev/hackathons/2024-google-cloud-japan-ai-hackathon">Zenn 主催のハッカソン</a> にも参加する予定なので、
        2月中旬まではそっちに集中する必要がありそう。</p>
        <p>相変わらず予定を詰め込む癖があります。</p>
        <h3>勉強</h3>
        <ul>
        <li>英語
        <ul>
        <li>初心に帰って英検、英語日記、NativeCampあたりをやる予定</li>
        </ul>
        </li>
        <li>数学
        <ul>
        <li>微積、統計、線形代数あたりを復習したい</li>
        </ul>
        </li>
        <li>Rust
        <ul>
        <li>まだ何もやってない</li>
        <li>本は買ってある</li>
        </ul>
        </li>
        <li>Kubernetes
        <ul>
        <li>本は買ってある</li>
        <li>自宅サーバーを k8s 化したい</li>
        </ul>
        </li>
        <li>RAG とか AI 関係
        <ul>
        <li>技術の進歩が早すぎて驚くばかり。AIエージェント（定義が曖昧だが）とかも作ってみたい</li>
        </ul>
        </li>
        </ul>
        <h3>DIY とか絵とか</h3>
        <p>家のサイズが 1/3 くらいになったので趣味用の机がなくなってしまいました。
        近所のコワーキングスペースが時間貸しでそれなりに安そうなので活用しようと思います。</p>
        <h3>論文</h3>
        <p>去年の１１月から情報処理学会に入って少し論文を読んでいます。
        自分でも書きたいので試行錯誤中</p>
        ]]>
      </content:encoded>
      <pubDate>Tue, 07 Jan 2025 15:00:01 GMT</pubDate>
    </item>
    <item>
      <title>Deno, Web Streams API で Local LLM と音声通話するWebシステムを開発している</title>
      <link>https://www.kbaba1001.com/posts/202412140705_making-a-talk-with-local-llm-system-with-deno-web-streams-api/</link>
      <guid isPermaLink="false">https://www.kbaba1001.com/posts/202412140705_making-a-talk-with-local-llm-system-with-deno-web-streams-api/</guid>
      <content:encoded>
        <![CDATA[<h2>開発中のもの</h2>
        <iframe width="560" height="315" src="https://www.youtube.com/embed/LLjdWHjzwQU?si=YYHC0dihp1bLUwsT" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        <p>最近時々見かけるようになった、AIと音声通話できるやつを作ってみた。</p>
        <p>既存サービスとの違い、特徴としては</p>
        <ul>
        <li>外部APIを使わず、自宅サーバーのローカルGPUのみで動作</li>
        <li>Web Streams API を活用してリアルタイム処理を実現</li>
        <li>Backend: Deno, Hono, Kysely, PostgreSQL</li>
        <li>Frontend: Node, Vite, React</li>
        </ul>
        <h2>大まかなロジック</h2>
        <ul>
        <li>人間の声をブラウザのマイク機能で取得してサーバーに Web Streams で送る</li>
        <li>サーバー側で音声データを Faster Whisper を用いてテキスト化する</li>
        <li>テキストをリアルタイムにフロントエンドで取得</li>
        <li>テキストをローカルLLM (Ollama) にプロンプトとして渡す</li>
        <li>ローカルLLMからの実行結果をリアルタイムにフロントエンドに送信</li>
        <li>フロントエンドでテキストを読み上げ</li>
        </ul>
        <h2>リアルタイム処理</h2>
        <p>同期処理とかストリーミング処理などともいう。
        フロントエンドとバックエンドでデータのやり取りをする際に、少しずつデータを送ることでタイムラグを少なくしてシステムが動くようにする技術のこと。</p>
        <h3>リアルタイム処理の実装方法</h3>
        <p>Webシステムのリアルタイム処理について調べると WebSocket に関する情報がたくさん出てくる。</p>
        <h4>WebSocket のメリット・デメリット</h4>
        <ul>
        <li>WebSocket のメリット
        <ul>
        <li>sockert io などのライブラリを使うと簡単に実装できる</li>
        <li>古いブラウザでも動く</li>
        <li>使っている人が多いのかドキュメントが豊富</li>
        </ul>
        </li>
        <li>WebSocket のデメリット
        <ul>
        <li>双方向通信なので通信量が多い</li>
        <li>ws/wssというプロトコルを使うのでサーバー側で http/https とは別の設計が必要になる</li>
        </ul>
        </li>
        </ul>
        <h4>WebStreams のメリット・デメリット</h4>
        <ul>
        <li>WebStreams のメリット
        <ul>
        <li>単方向通信なので通信量を抑えることができる</li>
        <li>http/https で扱うことができるのでサーバーに特別な実装が不要</li>
        </ul>
        </li>
        <li>WebStreams のデメリット
        <ul>
        <li>使ってる人が少ないのかドキュメントが少ない（ほとんどMDNだけかも）</li>
        <li>古いブラウザでは動かないことがある</li>
        </ul>
        </li>
        </ul>
        <h4>今回は WebStreams を採用</h4>
        <p>WebStreams を使ったことがなかったのでやってみたかったというのが一番大きな理由（というかそのためにこのシステムを作っている）なのだが、WebSocketsは意外と気難しいやつだと思っていて、一見気楽に実装できて良いのだが本番環境まで含めて考えると通信量などの問題もあり使いこなすのが難しいという感触がある。</p>
        <h2>WebStreams API とは何か？</h2>
        <ul>
        <li>Readable Stream, Writable Stream, Transform Stream を使ってデータのやり取りができる。</li>
        <li>これらの Stream はシェルコマンドのように pipe してつなげることができるため、すっきりしたコードを書くことができて嬉しい。</li>
        <li>Web標準の機能であるから今どきのブラウザ、そしてDenoで活用することができる
        <ul>
        <li>Node.js は歴史的な経緯により Deno ほど綺麗な実装になっていない</li>
        </ul>
        </li>
        </ul>
        <p>詳しくは MDN 参照 <a href="https://developer.mozilla.org/en-US/docs/Web/API/Streams_API">https://developer.mozilla.org/en-US/docs/Web/API/Streams_API</a></p>
        <h2>WebStreams API はどこで使われているか？</h2>
        <ul>
        <li><a href="https://developer.mozilla.org/en-US/docs/Web/API/Response">fetch の Response Body</a></li>
        <li><a href="https://hono.dev/docs/helpers/streaming">Hono の Streaming Helper</a></li>
        <li>Deno の IO や HTTP リクエストなど</li>
        </ul>
        <h2>作成するシステムの概要</h2>
        <p><img src="https://www.kbaba1001.com/img/posts/202412140705/design.png" alt="システムの概要図"></p>
        <h3>人の声をテキスト化する部分</h3>
        <p>フロント側の処理。音声データを取得してバックエンドに ReadableStream を使って送信する。</p>
        <pre><code class="language-ts">import { jwtTokenAtom } from &quot;@/atoms/current-user&quot;;
        import { httpClient } from &quot;@/libs/http-client&quot;;
        import { Box, Button } from &quot;@chakra-ui/react&quot;;
        import { useMutation } from &quot;@tanstack/react-query&quot;;
        import { useAtom } from &quot;jotai&quot;;
        import { useRef } from &quot;react&quot;;
        
        type AudioStreamerProps = {
        talkId: string;
        };
        
        export const AudioStreamer = ({ talkId }: AudioStreamerProps) =&gt; {
        // 音声データの取得用の Ref
        const mediaRecorderRef = useRef&lt;MediaRecorder | null&gt;(null);
        const streamControllerRef =
        useRef&lt;ReadableStreamDefaultController&lt;Uint8Array&gt; | null&gt;(null);
        const [jwtToken] = useAtom(jwtTokenAtom);
        
        const { mutate, isPending } = useMutation({
        mutationFn: async (audioStream: ReadableStream&lt;Uint8Array&gt;) =&gt; {
        return await httpClient({ jwtToken })
        .post(`talks/${talkId}/stream`, {
        body: audioStream,
        timeout: false,
        })
        .text();
        },
        onSuccess: (data) =&gt; {
        console.log(data);
        },
        onError: (error) =&gt; {
        console.error(error);
        },
        });
        
        // 音声データを取得してサーバーにリクエストする
        const startRecording = async () =&gt; {
        try {
        // マイクへのアクセスをリクエスト
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        
        // MediaRecorderの作成
        const options = {
        mimeType: &quot;audio/webm; codecs=opus&quot;, // サポートされているmimeTypeを指定
        };
        const mediaRecorder = new MediaRecorder(stream, options);
        
        mediaRecorderRef.current = mediaRecorder;
        
        // ReadableStreamの作成
        const audioStream = new ReadableStream&lt;Uint8Array&gt;({
        start(controller) {
        // コントローラを保存して、後でデータをエンキューする
        streamControllerRef.current = controller;
        },
        });
        
        // dataavailableイベントのハンドリング
        mediaRecorder.addEventListener(&quot;dataavailable&quot;, async (event) =&gt; {
        if (event.data &amp;&amp; event.data.size &gt; 0) {
        // BlobをArrayBufferに変換
        const arrayBuffer = await event.data.arrayBuffer();
        // ArrayBufferをUint8Arrayに変換
        const chunk = new Uint8Array(arrayBuffer);
        // ReadableStreamにチャンクをエンキュー
        streamControllerRef.current?.enqueue(chunk);
        }
        });
        
        // stopイベントのハンドリング
        mediaRecorder.addEventListener(&quot;stop&quot;, () =&gt; {
        // ReadableStreamをクローズ
        streamControllerRef.current?.close();
        });
        
        // 録音開始（100msごとにデータを収集）
        mediaRecorder.start(100);
        
        // ReadableStreamをAPIに送信
        mutate(audioStream);
        } catch (error) {
        console.error(&quot;マイクへのアクセスエラー:&quot;, error);
        }
        };
        
        const stopRecording = () =&gt; {
        mediaRecorderRef.current?.stop();
        };
        
        return (
        &lt;Box&gt;
        &lt;Button colorScheme=&quot;red&quot; onClick={startRecording} isLoading={isPending}&gt;
        録音
        &lt;/Button&gt;
        &lt;Button
        colorScheme=&quot;blue&quot;
        onClick={stopRecording}
        isDisabled={!isPending}
        &gt;
        停止
        &lt;/Button&gt;
        &lt;/Box&gt;
        );
        };
        </code></pre>
        <p>バックエンドでは、ReadableStreamを受け取って <a href="https://github.com/ufal/whisper_streaming">whisper_streaming</a> のサーバー（これは別途立ち上げておく）に音声データを渡し、テキストデータを受け取る。</p>
        <pre><code class="language-ts">app.post(&quot;/:id/stream&quot;, permissionChecker(&quot;talks&quot;), async (c) =&gt; {
        // 音声データをReadableStreamで取得する
        const stream = c.req.raw.body as ReadableStream;
        const talkId = Number(c.req.param(&quot;id&quot;));
        const user = c.get(&quot;currentUser&quot;);
        
        if (stream) {
        // whisper streaming は サンプリングレートなどに指定があるため FFmpeg で変換する
        // これは Web Streams API でパイプできる
        const ffmpeg = new Deno.Command(&quot;ffmpeg&quot;, {
        args: [
        &quot;-i&quot;,
        &quot;pipe:0&quot;, // 標準入力からデータを受け取る
        &quot;-ar&quot;,
        &quot;16000&quot;, // サンプリングレートを16000Hzに設定
        &quot;-ac&quot;,
        &quot;1&quot;, // モノラルに設定
        &quot;-sample_fmt&quot;,
        &quot;s16&quot;, // サンプルフォーマットをs16に設定
        &quot;-f&quot;,
        &quot;wav&quot;, // 出力フォーマットをWAVに設定
        &quot;pipe:1&quot;, // 標準出力にデータを出力
        ],
        stdin: &quot;piped&quot;,
        stdout: &quot;piped&quot;,
        stderr: &quot;piped&quot;,
        });
        const ffmpegProcess = ffmpeg.spawn();
        
        stream.pipeTo(ffmpegProcess.stdin);
        
        const convertedAudioStream = ffmpegProcess.stdout;
        
        // WhipserStreamingServer は TCP サーバーなので Deno.connect で接続できる
        // これも Web Streams でパイプできる
        const whisper = await Deno.connect({
        hostname: Deno.env.get(&quot;WHISPER_HOST&quot;) || &quot;localhost&quot;,
        port: Number(Deno.env.get(&quot;WHISPER_PORT&quot;)) || 43001,
        });
        
        convertedAudioStream.pipeTo(whisper.writable);
        
        const reader = whisper.readable.getReader();
        const decoder = new TextDecoder(&quot;utf-8&quot;);
        let sentence = &quot;&quot;;
        const sentenceIdentifier = crypto.randomUUID();
        
        try {
        // Streams からテキストデータを受け取って、DBに保存したり通知したりする
        while (true) {
        const { done, value } = await reader.read();
        if (done) {
        break;
        }
        const text = decoder.decode(value);
        // 先頭に数字が入っているので削除
        const wordText = text.replace(/^[\d\s]+/, &quot;&quot;).replace(/\n/g, &quot;&quot;);
        
        if (wordText.length &gt; 0) {
        sentence += wordText;
        // 後述のGETリクエストでテキストデータを取得できるようにするためにNotifyする
        await notify(&quot;words_inserted&quot;, {
        id: crypto.randomUUID(),
        talkId,
        userId: user.id,
        words: wordText,
        sentenceIdentifier,
        });
        await modelWords.create({
        talkId,
        userId: user.id,
        words: wordText,
        sentenceIdentifier,
        });
        }
        }
        } catch (error) {
        if (error instanceof Deno.errors.Interrupted) {
        customLogger(&quot;POST クライアントが接続を閉じました。&quot;);
        }
        }
        
        return c.text(&quot;completed&quot;, 200);
        }
        
        return c.text(&quot;ストリームが存在しません&quot;, 400);
        });
        </code></pre>
        <h3>新しいテキストができた時にフロントで取得する部分</h3>
        <p>フロントではAIと話すページを開いたときにバックエンドに対してGETリクエストを出して、
        過去の会話履歴を取得するようにしている。
        このリクエストをStreamで繋いだままにしておいて、新しいデータが入れば取得するようにしておく</p>
        <pre><code class="language-ts">	const [words, setWords] = useState&lt;Word[]&gt;([]);
        
        useEffect(() =&gt; {
        const fetchStream = async () =&gt; {
        // 次のエンドポイントはReadableStreamを返す
        const response = await httpClient({ jwtToken }).get(
        `talks/${talkId}/stream`,
        );
        const stream = response.body;
        
        if (!stream) {
        console.error(&quot;このブラウザはストリーミングをサポートしていません。&quot;);
        return;
        }
        
        const reader = stream.getReader();
        const decoder = new TextDecoder(&quot;utf-8&quot;);
        try {
        while (true) {
        const { done, value } = await reader.read();
        if (done) {
        console.log(&quot;done!&quot;);
        break;
        }
        
        // 新しいデータを受け取ったら Words 配列にいれる
        // (なるべく連続するデータは同じフキダシに入れたいのでちょっとコードがごちゃごちゃしている)
        const chunk = decoder.decode(value, { stream: true });
        const json = JSON.parse(chunk);
        const lastWord = json[0];
        if (lastWord?.assistantId != null) {
        speak(lastWord.words);
        }
        setWords((prev) =&gt; {
        const newArray = [...json, ...prev];
        const result = newArray.reduce((acc, cur) =&gt; {
        const key = cur.sentenceIdentifier;
        acc[key] = {
        id: key,
        userId: cur.userId,
        talkId: cur.talkId,
        words: acc[key]
        ? `${cur.words}${acc[key].words}`.replace(/\n/g, &quot;&quot;)
        : cur.words.replace(/\n/g, &quot;&quot;),
        sentenceIdentifier: key,
        };
        
        return acc;
        }, {});
        return Object.values(result);
        });
        }
        } catch (error) {
        console.error(&quot;ストリームの読み取り中にエラーが発生しました:&quot;, error);
        }
        };
        
        fetchStream();
        }, [jwtToken, talkId]);
        
        // words 配列を表示する処理が続く
        </code></pre>
        <p>バックエンドでは PostgreSQL の <a href="https://www.postgresql.org/docs/current/sql-notify.html">Notify/Listen</a> 機能を使って
        Channel のようなことをする。
        これにより、新しいデータが追加されたらフロントエンドに送る。</p>
        <pre><code class="language-ts">app.get(&quot;/:id/stream&quot;, permissionChecker(&quot;talks&quot;), async (c) =&gt; {
        const talkId = Number(c.req.param(&quot;id&quot;));
        
        const words = await modelWords.findAllByTalkId(talkId);
        const wordsJson = JSON.stringify(words);
        
        // stream を返すのでクライアントが接続を切らない限りデータを渡し続けることができる。
        return streamText(c, async (s) =&gt; {
        await s.writeln(wordsJson);
        // words_insterted という channel にJSONテキストが入ってくるのでそれを監視する
        // WARN channel 名に talkId を入れたほうが他の会話のデータを誤って取得せずに済みそう
        await listen(&quot;listen words_inserted&quot;, async (msg) =&gt; {
        if (msg.channel !== &quot;words_inserted&quot;) {
        return;
        }
        
        const word = JSON.parse(msg.payload);
        await s.write(JSON.stringify([word]));
        });
        
        s.onAbort(() =&gt; {
        customLogger(&quot;GET クライアントが接続を閉じました。&quot;);
        });
        
        while (true) {
        await s.sleep(1000);
        }
        });
        });
        </code></pre>
        <p>listen と notify 関数の実装は次のような感じ</p>
        <pre><code class="language-ts">import { CamelCasePlugin, Kysely, PostgresDialect, sql } from &quot;kysely&quot;;
        import Pool from &quot;pg-pool&quot;;
        import type { DB } from &quot;./database-types.ts&quot;;
        import { clerk } from &quot;./libs/clerk.ts&quot;;
        
        export const pool = new Pool({
        connectionString:
        Deno.env.get(&quot;DENO_ENV&quot;) === &quot;test&quot;
        ? Deno.env.get(&quot;TEST_DB_URL&quot;)
        : Deno.env.get(&quot;DATABASE_URL&quot;),
        max: 20,
        idleTimeoutMillis: 30000,
        connectionTimeoutMillis: 2000,
        });
        
        const dialect = new PostgresDialect({
        pool,
        });
        
        export const db = new Kysely&lt;DB&gt;({
        dialect,
        plugins: [new CamelCasePlugin()],
        log: (log) =&gt; {
        if (log.level === &quot;query&quot;) {
        clerk.info(&quot;SQL&quot;, log.query.sql);
        }
        },
        });
        
        export async function notify(channel: string, obj: object) {
        return await sql`select pg_notify(${channel}, ${JSON.stringify(obj)})`.execute(
        db,
        );
        }
        
        export async function listen(
        listenChannel: string,
        callback: (msg: { channel: string; payload: string }) =&gt; void,
        ) {
        const pgClient = await pool.connect();
        pgClient.on(&quot;notification&quot;, callback);
        
        return await pgClient.query(listenChannel);
        }
        </code></pre>
        <h3>Local LLM へのリクエストとレスポンス</h3>
        <p>会話データができたので、これをもとにプロンプトを作成してローカルLLMにリクエストする。</p>
        <pre><code class="language-ts">import { Button } from &quot;@chakra-ui/react&quot;;
        
        export function TalkToAI({
        assistantMutate,
        assistantIsPending,
        }: { assistantMutate: () =&gt; void; assistantIsPending: boolean }) {
        return (
        &lt;Button
        colorScheme=&quot;blue&quot;
        onClick={() =&gt; assistantMutate()}
        isLoading={assistantIsPending}
        &gt;
        AIをよびだす
        &lt;/Button&gt;
        );
        }
        
        // assistantMutate の中身
        const { mutate: assistantMutate, isPending: assistantIsPending } =
        useMutation({
        mutationFn: async () =&gt;
        await httpClient({ jwtToken }).post(`talks/${talkId}/assistant_chat`, {
        timeout: false,
        }),
        onSuccess: () =&gt; {
        //
        console.log(&quot;ok&quot;);
        },
        onError: (error) =&gt; {
        console.error(error);
        },
        });
        </code></pre>
        <p>現状、AI呼び出しのトリガーはボタンにしてある。
        将来的にはユーザーが話し終わったら自動で処理するようにしたい。</p>
        <p>バックエンドでは <code>talks/${talkId}/assistant_chat</code> にリクエストがあると、次の処理を行う。</p>
        <ul>
        <li>次の情報からLLMのプロンプトを作成する
        <ul>
        <li>直近数回の人間とAIの会話データ</li>
        <li>AIの人格設定データ</li>
        </ul>
        </li>
        <li>ローカルLLMサーバー (Ollama を使用) にプロンプトを投げる</li>
        <li>レスポンスをStreamで受け取り、DBにNotify、保存</li>
        </ul>
        <pre><code class="language-ts">app.post(&quot;/:id/assistant_chat&quot;, permissionChecker(&quot;talks&quot;), async (c) =&gt; {
        const talkId = Number(c.req.param(&quot;id&quot;));
        const user = c.get(&quot;currentUser&quot;);
        const talk = await modelTalks.findById({
        talkId,
        userId: user.id,
        });
        
        if (!talk) {
        return c.json({ message: &quot;Talk not found&quot; }, 404);
        }
        
        // 直近数回の会話を取得
        const words = await modelWords.findLatestByTalkId(talkId);
        
        // 人間とAIの会話によって role を切り替えながらLLMのプロンプトを作成
        let messages = words
        .map((word) =&gt; ({
        role: word.userId == null ? &quot;assistant&quot; : &quot;user&quot;,
        content: `${word.words}`,
        }))
        .reverse();
        
        // AIの人格設定は system role で渡す
        messages = [
        ...messages,
        { role: &quot;system&quot;, content: talk.assistant.personality },
        ];
        
        // Ollama で動いているローカルLLMにリクエストする
        const response = await ky.post(`${Deno.env.get(&quot;OLLAMA_URL&quot;)}/api/chat`, {
        json: {
        model: talk.assistant.model,
        messages,
        stream: true,
        },
        });
        
        // ローカルLLMサーバーからのレスポンスをchannelに通知したり、保存したりする
        const reader = response.body?.getReader();
        const decoder = new TextDecoder(&quot;utf-8&quot;);
        const sentenceIdentifier = crypto.randomUUID();
        let text = &quot;&quot;;
        let textChank = &quot;&quot;;
        while (reader &amp;&amp; true) {
        const { done, value } = await reader.read();
        if (done) {
        break;
        }
        const jsonText = decoder.decode(value, { stream: true });
        const json = JSON.parse(jsonText);
        
        textChank += json.message.content;
        if (textChank.length &gt; 20) {
        await notify(&quot;words_inserted&quot;, {
        id: crypto.randomUUID(),
        talkId: talk.id,
        assistantId: talk.assistantId,
        words: textChank,
        sentenceIdentifier,
        });
        
        text += textChank;
        textChank = &quot;&quot;;
        }
        }
        
        if (textChank.length &gt; 0) {
        await notify(&quot;words_inserted&quot;, {
        id: crypto.randomUUID(),
        talkId: talk.id,
        assistantId: talk.assistantId,
        words: textChank,
        sentenceIdentifier,
        });
        
        text += textChank;
        textChank = &quot;&quot;;
        
        await modelWords.create({
        talkId,
        assistantId: talk.assistantId,
        words: text,
        sentenceIdentifier,
        });
        }
        
        return c.json({ msg: text });
        });
        </code></pre>
        <h3>AIからのレスポンステキストの読み上げ</h3>
        <p>ひとまずブラウザの機能で読み上げているだけ</p>
        <pre><code class="language-ts">function speak(text: string) {
        // SpeechSynthesisUtteranceのインスタンスを作成
        const utterance = new SpeechSynthesisUtterance();
        utterance.text = text;
        utterance.lang = &quot;ja-JP&quot;;
        // 音声を再生
        window.speechSynthesis.speak(utterance);
        }
        </code></pre>
        <p>新しいテキストを取得した際に、AIからの発言であれば上記の関数を使って読み上げる。</p>
        <h2>今後の展望</h2>
        <p>テキスト読み上げが地味なので <a href="https://github.com/VOICEVOX/voicevox_engine">VOICEVOX</a> などをつかって音声データをサーバーで作成し、
        フロントに送るようにしたい。
        これもStreamを利用してリアルタイムにやりたいので、そんな感じの仕組みを作成中。</p>
        <p>すべてのソースコードの公開は全体が完成したらします。</p>
        ]]>
      </content:encoded>
      <pubDate>Sat, 14 Dec 2024 07:05:34 GMT</pubDate>
    </item>
    <item>
      <title>Clojure Family が色々ある話</title>
      <link>https://www.kbaba1001.com/posts/202412030751_clojure-family/</link>
      <guid isPermaLink="false">https://www.kbaba1001.com/posts/202412030751_clojure-family/</guid>
      <content:encoded>
        <![CDATA[<p>Clojure といえば JVM 上で動く Lisp のように紹介されることが多く、
        私もそういう言い方をすることも多いのだがこの言い方は正しくない。
        なぜなら JVM 以外で動く Clojure (あるいは Clojure っぽい言語も含む) が多々存在しているからだ。</p>
        <p>今日は Clojure Family とも言うべきこれらの方言について紹介する。</p>
        <h2>Clojure (JVMで動くやつ)</h2>
        <p>まずは一番良く使われている JVM 上で動く Clojure 。
        特筆されない場合 &quot;Clojure&quot; といえばこれのこと。</p>
        <ul>
        <li>Java の機能やエコシステムを活用したい場合はとてもよい</li>
        <li>とりあえず Clojure を使いたい場合はこれがおすすめ</li>
        </ul>
        <h2>ClojureScript (Alt JS の一種)</h2>
        <p>ビルドすると JavaScript になる Clojure 。
        おそらく JVM 版の次によく使われている。
        JVM 版と同じところが開発しているという安心感がある。</p>
        <p>ただ、ビルドツールが色々とあって、</p>
        <ul>
        <li><a href="https://clojurescript.org/guides/quick-start">deps.ednを使う方法</a></li>
        <li><a href="https://figwheel.org/">Figwheel</a></li>
        <li><a href="https://github.com/thheller/shadow-cljs">Shadow-cljs</a></li>
        </ul>
        <p>などのやり方がある。</p>
        <p>「deps.edn を使う方法」は公式サイトに乗っているやり方だが、これは npm ライブラリを素直に使えないというデメリットが有り、
        そのへんを解決するために後者の 2 つがある。</p>
        <p>Shadow-cljs が一番後発なので、今から新しく何かを作りたいなら、これがおすすめ。
        （あるいは後述の squint や cherry を使う）</p>
        <h2>ClojureCLR</h2>
        <p>こちらも Clojure 本家が作成しているもので、 ビルドすると Microsoft’s .Net Framework 環境で動く Clojure 。
        つまり C# で書かれた Clojure 実装。</p>
        <p>私は使ったことがなく、詳しくないので紹介する程度に留める。
        ゲームとかWindowsサーバーでClojureを使いたい人にはいいのかもしれない。</p>
        <h2>Babashka</h2>
        <p><a href="https://babashka.org/">Babashka</a> は GraalVM を用いて Clojure を native binary にビルドできるもの。</p>
        <p>JVM 版の Clojure を使うとどうしても実行時の起動に時間が掛かるが、 Babashka であればすぐに動くので
        スクリプトを Clojure で書きたい場合などに便利。</p>
        <p><a href="https://github.com/borkdude">Michiel Borkent</a> 氏の作品。彼は Babashka を始め後述の様々なツールを作っているすごい人。</p>
        <h2>Squint</h2>
        <p><a href="https://github.com/squint-cljs/squint">squint</a> は ClojureScript のコンパイラなのだが、 JavaScript の Built-in Data structures のみ使用しているため、いくつかの点で本来の ClojureScript とは異なる。</p>
        <ul>
        <li>ClojureScript では <code>[]</code> や <code>{}</code> は JavaScript のオブジェクトではなく immutable な ClojureScript ようにカスタムされたオブジェクトを返すが、 Squint では JavaScript の build-in の <code>Array</code> や <code>Object</code> を返す。</li>
        <li>Async/Await のサポート (ClojureScript では core.async などを使う必要があったが Squint では Promise が素直に使える)</li>
        <li>ClojureScript 用に開発されたライブラリなどがうまく動かない場合があるので、JS/TS用のライブラリを使うほうが無難</li>
        <li><a href="https://github.com/brandonstubbs/vite-plugin-squint">vite-plugin-squint</a> などを使うと Squint -&gt; JSX -&gt; JS の変換が楽</li>
        </ul>
        <p>Squint はないよりも気軽に CLJS （っぽい言語）が使えるのが魅力。
        私としてはかなり期待している。</p>
        <h2>Cherry</h2>
        <p><a href="https://github.com/squint-cljs/cherry">Cherry</a> は Squint よりもともとの ClojureScript への互換性を高めたもの。
        ClojureScript のライブラリを活用したい場合はこちらのほうが良さそう。</p>
        <h2>Jank</h2>
        <p><a href="https://jank-lang.org/">Jank</a> は Clojure を LLVM + JIT コンパイラで動くようにしたもの。
        C++との親和性が高いらしく、C/C++ っぽい領域のところを Clojure で書きたい場合には良さそう。
        どのくらいの事ができるのかをまだ私は把握しきれていないけど、将来性はあるように思うので期待。</p>
        <h2>その他</h2>
        <p>私は使ったことないけど他のClojure方言を紹介</p>
        <ul>
        <li><a href="https://github.com/Tensegritics/ClojureDart">ClojureDart</a></li>
        <li><a href="https://github.com/clojerl/clojerl">ClojureL</a>
        <ul>
        <li>Erlang 実装</li>
        </ul>
        </li>
        <li><a href="https://github.com/kanaka/mal">mal</a>
        <ul>
        <li>Clojure っぽい言語を実装することで言語実装について学ぶプロジェクト</li>
        </ul>
        </li>
        </ul>
        <h2>感想</h2>
        <p>Lisp は方言がたくさんある言語だけど、最近はもうClojure方言と呼ぶべき分野ができていて、
        やっぱりClojureの人気はすごいなぁと思ったりしました。
        Scheme方言なんかも結構たくさんあるみたいで調べてみると面白かったです。</p>
        ]]>
      </content:encoded>
      <pubDate>Tue, 03 Dec 2024 07:51:05 GMT</pubDate>
    </item>
    <item>
      <title>渋谷でもくもく会に参加した</title>
      <link>https://www.kbaba1001.com/posts/202411030733_attended-shibuya-mokumoku/</link>
      <guid isPermaLink="false">https://www.kbaba1001.com/posts/202411030733_attended-shibuya-mokumoku/</guid>
      <content:encoded>
        <![CDATA[<p><img src="https://www.kbaba1001.com/img/posts/202411030733/mokumoku.jpg" alt="渋谷でもくもく会"></p>
        <p><a href="https://connpass.com/event/334713/">渋谷でもくもく会</a> に参加してきた。</p>
        <p>こういうイベントに参加するのは久しぶりで楽しかった。</p>
        <h3>会の雰囲気</h3>
        <p>シェアオフィスでの開催だったので静かな空間で良かった。
        参加者も１０人くらいで適度におしゃべりしたりするのにちょうどいい感じ。</p>
        <p>もくもく会だが、作業したり喋ったりゆるい雰囲気で良かった。
        （ずっと黙っている会もあったりするが、自分好みではない）</p>
        <p>他の参加者は若手と中堅のエンジニアの方が半々くらいで、
        適度に会話が盛り上がって楽しかった。
        意外と女性の参加者も多かった。</p>
        <p>私は最近静岡に引きこもっているので、Tech系の話をするのは久々で
        自分がこういうおしゃべりに飢えていたことに気づいた。</p>
        <p>若手エンジニアの方たちの話は自分にとっては新鮮なところもあって、
        転職の話とかどのように技術を学んでいくかとか、話題が若くて良かった。</p>
        <h3>やっていたこと</h3>
        <p>最近 ChatGPT に AI と音声で会話できる機能がついたのだが、それと同じようなものを
        ローカルLLMを使って実現したい。次の流れで実現できると思っている。</p>
        <ul>
        <li>ユーザーの音声のテキスト化
        <ul>
        <li>ユーザーの音声をブラウザで取得</li>
        <li>音声データをStreamでサーバーにおくる</li>
        <li>サーバーでは音声データをWhisperで文字に変換する (ここもStream処理)</li>
        <li>文字データをフロント側にStreamで返却しつつ、DBにも保存する</li>
        </ul>
        </li>
        <li>テキストをローカルLLMにわたして返答をつくる
        <ul>
        <li>１００文字くらいの短い文章を作るように命令しておく</li>
        <li>インプットはStreamではなくまとめてテキストデータを入れる</li>
        <li>アウトプットをStream で受け取る</li>
        </ul>
        </li>
        <li>LLMの返答を音声データにする
        <ul>
        <li>LLMのアウトプットを Stream で受け取り、それを <a href="https://github.com/KoljaB/RealtimeTTS">RealtimeTTS</a> などを用いて Stream で音声データにする</li>
        <li>サーバーからフロントに音声データと文字データを Stream で送る</li>
        <li>フロントではサーバーからのレスポンスを Stream で受け取りつつ、音声を再生し、文字データを画面に表示する</li>
        </ul>
        </li>
        </ul>
        <p>もくもく会では、ブラウザ上で音声データを取得して ReadableStream でサーバーに送り、
        サーバー側でファイル化するという処理を作ることができた。</p>
        <p>実際には受け取ったバイナリをファイルにするんじゃなくて、そのまま <a href="https://github.com/ufal/whisper_streaming/blob/main/whisper_online_server.py">whisper_online_server.py</a> に渡してテキスト化したい。</p>
        <h3>結果</h3>
        <p>会場ではかなり集中して作業できてきちんとした進捗が出せてよかった。</p>
        <p>12:00 - 18:30 （わたしは 17:30 で帰ったけど）という長時間の会だったので、
        なかなか普段ここまでまとまった時間は取れないので、それだけでもありがたかった。</p>
        <h3>蛇足: わからないことをAIに聞くことに慣れた</h3>
        <p>会とは関係ないけど WebStream API をつかったコーディングは初めてで
        MDNのサンプルコードだけだとしんどいなぁと思っていたところ、
        ChatGPTにサンプルコードを提示させるとかなりいいコードがでてきて、
        なんかもうこれでいいじゃんとなった。
        こういう規格に沿ったものをつくるのはAIのほうが得意そうだ。</p>
        <p>バックエンドも Deno にしているので Web 標準の機能が使えてスッキリするが、
        これも資料が少ないので ChatGPT に聞いたほうが何かと早い。。。</p>
        <p>ChatGPTの作ったコードをとりあえず動かしてみて、問題があったらドキュメントを参考に修正するという感じ。</p>
        <p>すごい時代になったなぁ。。。</p>
        ]]>
      </content:encoded>
      <pubDate>Sun, 03 Nov 2024 07:33:24 GMT</pubDate>
    </item>
  </channel>
</rss>